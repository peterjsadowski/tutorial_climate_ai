{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL3Ldy7A6Ycl"
      },
      "source": [
        "# ENSO Phase Prediction\n",
        "\n",
        "The El Niño Southern Oscillation (ENSO) is a climate phenomenon that occurs in the Pacific ocean. It is characterized by a periodic warming and cooling of the sea surface temperature. The ENSO phase can be classified into three categories: El Niño, La Niña, and Neutral. This event has a significant impact on weather patterns around the world, and is of great interest to climate scientists.\n",
        "\n",
        "### Sea Surface Temperature Data: NOAA Extended Reconstructed Sea Surface Temperature (ERSST) v5\n",
        "\n",
        "The NOAA Extended Reconstructed Sea Surface Temperature (ERSST) v5 dataset is a global monthly sea surface temperature dataset. It is a blend of in situ and satellite data that begins in 1854 and is updated monthly. The data is available in netCDF format from the NOAA website.\n",
        "\n",
        "A subset of the data has already been prepared for this workshop. In particular, we will focus our attention on a region in the Pacific ocean (40S-40N, 120E-100W) from 1854-2023. More specifically, we will use the sea surface temperature anomaly (SSTA) data, which is the difference between the sea surface temperature and the long-term average. This data is stored in a 3D array (time, lat, lon) in netCDF format, which is a common format for climate data.\n",
        "\n",
        "NetCDF is a common data format for climate data. It is a self-describing format that is optimized for storing large arrays. It is supported by most programming languages and has a number of useful tools for working with the data. See the NetCDF website for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawaii-ai/tutorial_climate_ai/blob/main/02_classify_ENSO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qS74CLw6Ycn"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "To start, import all the relevant libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EImUVsNx6Ycn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import urllib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import xarray as xr\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we select a device to train the model on. If you are using Google Colab, you can use a GPU by selecting Runtime -> Change runtime type -> Hardware accelerator -> GPU. If you are using your own computer, you can use a GPU if you have one available. Otherwise, you can use the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Hkq_L8L-6Yco"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.device(\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v23fffGu6Ycp"
      },
      "source": [
        "## Download and Preprocess Data\n",
        "\n",
        "Download the data to the local environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory 'data' already exists.\n",
            "File data/ersst_pacific_anom.nc already exists.\n",
            "Error downloading file data/labels.csv: HTTP Error 404: Not Found\n",
            "File data/ersst_anom_dec2023.nc already exists.\n"
          ]
        }
      ],
      "source": [
        "# Create data dir.\n",
        "directory_path = \"data\"\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)\n",
        "    print(f\"Directory '{directory_path}' created.\")\n",
        "else:\n",
        "    print(f\"Directory '{directory_path}' already exists.\")\n",
        "\n",
        "# Download data files from github repo\n",
        "def download(url):\n",
        "    filename = os.path.join(directory_path, url.split('/')[-1])\n",
        "    if os.path.exists(filename):\n",
        "        print(f'File {filename} already exists.')\n",
        "    else:\n",
        "        # Try to download\n",
        "        try:\n",
        "            urllib.request.urlretrieve(url, filename)\n",
        "            print(f'File {filename} downloaded.')\n",
        "        except Exception as e:\n",
        "            print(f'Error downloading file {filename}: {e}')\n",
        "\n",
        "\n",
        "urls = ['https://github.com/peterjsadowski/tutorial_climate_ai/data/ersst_pacific_anom.nc',\n",
        "        'https://github.com/peterjsadowski/tutorial_climate_ai/data/labels.csv',\n",
        "        'https://github.com/peterjsadowski/tutorial_climate_ai/data/ersst_anom_dec2023.nc'\n",
        "]\n",
        "for url in urls:\n",
        "    download(url)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the data into an xarray dataset using the xr.open_dataset function and select the variable ssta (sea surface temperature anomaly). The data is a 3D array (time, lat, lon) with missing values. We will fill in the missing values with zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "w1jZTtPN6Ycp"
      },
      "outputs": [],
      "source": [
        "netcdf_file = xr.open_dataset(\"data/ersst_pacific_anom.nc\")\n",
        "ssta = (\n",
        "    netcdf_file.ssta.expand_dims(\"channel\")\n",
        "    .transpose(\"time\", \"channel\", \"lat\", \"lon\")\n",
        "    .fillna(0)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now load the labels for the ENSO phase which has been prepared for this workshop and can be downloaded from this link. The labels are stored in a pandas dataframe with 4 columns: date, lead_1, lead_2, lead_3. The date column is the date of the observation, while the lead columns are the ENSO phase 1, 2, and 3 months in the future. The ENSO phase has been converted to a categorical variable with 3 classes: 0 (Neutral), 1 (El Niño), and 2 (La Niña).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPDdI-zX6Ycs"
      },
      "outputs": [],
      "source": [
        "labels_df = pd.read_csv(\"data/labels.csv\")\n",
        "one_hot_labels = nn.functional.one_hot(\n",
        "    torch.from_numpy(labels_df[[\"lead_1\", \"lead_2\", \"lead_3\"]].values).long(),\n",
        "    num_classes=3,\n",
        ").float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One hot encoding is a method for converting categorical variables into numerical variables. For example, if we have a variable that can take on the values “red”, “green”, and “blue”, we can convert this into a numerical variable by assigning each category a number: 0, 1, and 2. However, this implies that “red” is closer to “green” than it is to “blue”, which is not true. One hot encoding solves this problem by creating a new variable for each category. In this case, we would have three variables: red, green, and blue. If the original variable was “red”, then the red variable would be 1 and the green and blue variables would be 0. If the original variable was “blue”, then the blue variable would be 1 and the red and green variables would be 0. This is a more appropriate representation of the data.\n",
        "\n",
        "```\n",
        "color\tRed\tGreen\tBlue\n",
        "Red\t1\t0\t0\n",
        "Green\t0\t1\t0\n",
        "Blue\t0\t0\t1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tiKsF4B6Ycs"
      },
      "source": [
        "## Define the Training Set\n",
        "\n",
        "Now that we have the data loaded, we need to prepare it in a format that Pytorch can use. We will define a dataset class that will take in the data and labels and return a single example. We will then use the Pytorch DataLoader class to create a data loader that will feed the data into the model in batches.\n",
        "\n",
        "In machine learning, it is critical to split the data into **training** and **test sets**. The training set is used to train the model, while the test set is used to evaluate the model. Machine learning models tend to memorize the training data, so the training error always goes to zero — the only way to know if the model **generalizes** to new data is by evaluating the performance on the test set, a method known as **cross-validation**.\n",
        "\n",
        "In timeseries data, there is a danger of **information leakage** where the training set is highly correlated to the test set (even if the exact examples are different). Thus, it is common use **chrono-cross-validation**, where the training data consists of examples before a certain time T, while the test set consists of examples after time T."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elqfisDV6Yct"
      },
      "outputs": [],
      "source": [
        "# now we have to create a dataset that can be used by pytorch\n",
        "\n",
        "class ENSODataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ssta, labels):\n",
        "        self.ssta = ssta\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ssta)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.ssta[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# we will use the first 80% of the data for training and the last 20% for testing\n",
        "train_size = int(0.8 * len(ssta))\n",
        "\n",
        "train_dataset = ENSODataset(ssta[:train_size].data.copy(), one_hot_labels[:train_size])\n",
        "test_dataset = ENSODataset(ssta[train_size:].data.copy(), one_hot_labels[train_size:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use a batch size of 32 and shuffle the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzA0r-0E6Yct"
      },
      "outputs": [],
      "source": [
        "# Build the dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv5iK3P86Yct"
      },
      "source": [
        "## Defining the Model\n",
        "\n",
        "Similar to the MNIST example, we will use a convolutional neural network (CNN) model as we are working with image data. As we want to predict the next 3 months of ENSO phase, we will have 3 output nodes (one for each lead time) each with 3 possible values (one for each ENSO phase). We will use the cross entropy loss function and the Adam optimizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQrMVnTn6Yct"
      },
      "outputs": [],
      "source": [
        "class ENSOModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        self.fc1 = nn.Linear(16 * 8 * 20, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "\n",
        "        x_lead_1 = self.fc3(x)\n",
        "        x_lead_2 = self.fc3(x)\n",
        "        x_lead_3 = self.fc3(x)\n",
        "\n",
        "        return x_lead_1, x_lead_2, x_lead_3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note on designing a neural network architecture:\n",
        "\n",
        "The neural network architecture can have a large impact on performance. The inductive bias of a model refers to the set of assumptions and prior beliefs that are inherent in this choice. All machine learning models have inductive bias. This is not a bad thing, but it means some models are more appropriate for some problems than others. This is formalized in what is known as The No Free Lunch Theorem.\n",
        "\n",
        "Experienced practicioners know which architectures tend to work best for certain types of problems. Bigger models usually work better, but are slower because of computational constraints. In practice, it is common to try lots of different models and perform model selection to pick the best based on how well the model performs on a test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3yMRUy_6Yct"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "Training the model is an iterative process. Starting from randomly initialized parameters, we iterate through the training examples, make predictions, and update the weight parameters to minimize the **loss function**. In this case, we use the cross entropy loss. We keep track of the average loss over the training set, and report it every time we iterate through the training dataset — each iteration through the training set is called an **epoch**.\n",
        "\n",
        "We can now write the training loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiwkeVS06Yct"
      },
      "outputs": [],
      "source": [
        "model = ENSOModel()\n",
        "\n",
        "# define the loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# train the model\n",
        "epochs = 60\n",
        "train_loss = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        output_lead_1, output_lead_2, output_lead_3 = model(data)\n",
        "        loss = (\n",
        "            loss_fn(output_lead_1, target[:, 0])\n",
        "            + loss_fn(output_lead_2, target[:, 1])\n",
        "            + loss_fn(output_lead_3, target[:, 2])\n",
        "        )\n",
        "\n",
        "        epoch_loss.append(loss.item() * data.size(0))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        if batch_idx % 400 == 0:\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}\")\n",
        "    train_loss.append(sum(epoch_loss) / len(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training loss should decrease during the first few epochs, indicating that the model is improving. If it doesn’t, try decreasing the learning rate. If that doesn’t help, then you have a bug or the model is not appropriate for your problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBsPbDL66Yct"
      },
      "outputs": [],
      "source": [
        "# plot the training loss\n",
        "plt.plot(train_loss)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPNdAxPP6Yct"
      },
      "source": [
        "## Evaluating the Model\n",
        "\n",
        "Now we can evaluate the model on the test set. We first set the model to evaluation mode, then we iterate through the test set and make predictions. We then compare the predictions to the ground truth labels and compute the accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxGjpYiG6Ycu"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "# test the accuracy per lead\n",
        "\n",
        "correct_lead_1 = 0\n",
        "correct_lead_2 = 0\n",
        "correct_lead_3 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_dataloader:\n",
        "        output_lead_1, output_lead_2, output_lead_3 = model(data)\n",
        "\n",
        "        _, predicted_lead_1 = torch.max(output_lead_1.data, 1)\n",
        "        _, predicted_lead_2 = torch.max(output_lead_2.data, 1)\n",
        "        _, predicted_lead_3 = torch.max(output_lead_3.data, 1)\n",
        "\n",
        "        _, target_lead_1 = torch.max(target[:, 0], 1)\n",
        "        _, target_lead_2 = torch.max(target[:, 1], 1)\n",
        "        _, target_lead_3 = torch.max(target[:, 2], 1)\n",
        "\n",
        "        correct_lead_1 += (predicted_lead_1 == target_lead_1).sum().item()\n",
        "        correct_lead_2 += (predicted_lead_2 == target_lead_2).sum().item()\n",
        "        correct_lead_3 += (predicted_lead_3 == target_lead_3).sum().item()\n",
        "\n",
        "# print the accuracy per lead in %\n",
        "print(f\"Accuracy Lead 1: {100 * correct_lead_1 / len(test_dataset):.2f}%\")\n",
        "print(f\"Accuracy Lead 2: {100 * correct_lead_2 / len(test_dataset):.2f}%\")\n",
        "print(f\"Accuracy Lead 3: {100 * correct_lead_3 / len(test_dataset):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q-ExMCN6Ycu"
      },
      "source": [
        "## Forecast with December 2023\n",
        "\n",
        "We can now use the model to make a forecast. We will use the December 2023 data as input to the model and see what the model predicts for the next 3 months. You can download the data from this link\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3UkztyK6Ycu"
      },
      "outputs": [],
      "source": [
        "dec2023 = xr.open_dataset(\"data/ersst_anom_dec2023.nc\").ssta\n",
        "dec2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzmEKWE36Ycu"
      },
      "outputs": [],
      "source": [
        "# evaluate the model on the dec2023 data\n",
        "model.eval()\n",
        "\n",
        "pred_input = torch.from_numpy(\n",
        "    dec2023.expand_dims(\"channel\")\n",
        "    .transpose(\"channel\", \"lat\", \"lon\")\n",
        "    .fillna(0)\n",
        "    .data.copy()\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    model_output = model(pred_input.unsqueeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLmd4aly6Ycu"
      },
      "outputs": [],
      "source": [
        "print(\"0: Neutral, 1: El Nino, 2: La Nina\")\n",
        "print(f\"Prediction for January 2024: {model_output[0].argmax()}\")\n",
        "print(f\"Prediction for February 2024: {model_output[1].argmax()}\")\n",
        "print(f\"Prediction for March 2024: {model_output[2].argmax()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Improving the model \n",
        "\n",
        "If we want to improve the model, the first thing to ask ourselves is whether we are **underfitting** or **overfitting**. If we are not overfitting, then we are underfitting and we should increase the size of the model and train longer. If we are overfitting, then we should add regularization or get more training data."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

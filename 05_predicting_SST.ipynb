{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ae99a2-b8bb-4239-bb34-4988eba79d00",
   "metadata": {},
   "source": [
    "# Predicting SST\n",
    "\n",
    "In this article, we use an autoregressive model to forecast SST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba40cd-9b67-4462-b741-d531122a0f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define a simple autoregressive model\n",
    "class AutoregressiveVideoModel(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels):\n",
    "        super(AutoregressiveVideoModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, hidden_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, output_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_channels, height, width)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x  # (batch_size, output_channels, height, width)\n",
    "\n",
    "# 2. Generate some dummy video data\n",
    "def generate_dummy_video_data(batch_size, num_frames, channels, height, width):\n",
    "    \"\"\"\n",
    "    Generates dummy video data for testing.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): The batch size.\n",
    "        num_frames (int): The number of frames in the video.\n",
    "        channels (int): The number of channels in each frame (e.g., 3 for RGB).\n",
    "        height (int): The height of each frame.\n",
    "        width (int): The width of each frame.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (batch_size, num_frames, channels, height, width).\n",
    "    \"\"\"\n",
    "    return torch.randn(batch_size, num_frames, channels, height, width)\n",
    "\n",
    "# 3. Prepare data for training\n",
    "def prepare_data(video_data):\n",
    "    \"\"\"\n",
    "    Prepares the video data for autoregressive training.  The input to the model\n",
    "    is a sequence of frames, and the target is the next frame.\n",
    "\n",
    "    Args:\n",
    "        video_data (torch.Tensor): Tensor of shape (batch_size, num_frames, channels, height, width).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (inputs, targets)\n",
    "            inputs:  Tensor of shape (batch_size, num_frames - 1, channels, height, width)\n",
    "            targets: Tensor of shape (batch_size, num_frames - 1, channels, height, width)\n",
    "    \"\"\"\n",
    "    inputs = video_data[:, :-1, :, :, :]\n",
    "    targets = video_data[:, 1:, :, :, :]\n",
    "    # Reshape to (batch_size * (num_frames - 1), ...) for easier training with 2D conv\n",
    "    inputs = inputs.reshape(-1, *inputs.shape[2:])\n",
    "    targets = targets.reshape(-1, *targets.shape[2:])\n",
    "    return inputs, targets\n",
    "\n",
    "# 4. Train the model\n",
    "def train_model(model, train_loader, optimizer, loss_fn, epochs=10):\n",
    "    \"\"\"\n",
    "    Trains the autoregressive video model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The autoregressive model.\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer.\n",
    "        loss_fn (nn.Module): The loss function.\n",
    "        epochs (int): The number of epochs to train. Defaults to 10.\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch: {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# 5.  Generate a future frame given a sequence of past frames\n",
    "def generate_next_frame(model, past_frames):\n",
    "    \"\"\"\n",
    "    Generates the next frame given a sequence of past frames.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained autoregressive model.\n",
    "        past_frames (torch.Tensor): Tensor of shape (1, sequence_length, channels, height, width).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The predicted next frame of shape (1, channels, height, width).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Use the last frame in the input sequence as the input to the model\n",
    "        next_frame = model(past_frames[:, -1])\n",
    "        return next_frame.unsqueeze(0) # Add the batch dimension back\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 0. Set random seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # 1. Hyperparameters\n",
    "    batch_size = 2\n",
    "    num_frames = 5\n",
    "    channels = 3  # RGB\n",
    "    height = 64\n",
    "    width = 64\n",
    "    hidden_channels = 16\n",
    "    epochs = 5\n",
    "\n",
    "    # 2. Generate dummy video data\n",
    "    video_data = generate_dummy_video_data(batch_size, num_frames, channels, height, width)\n",
    "\n",
    "    # 3. Prepare data for training\n",
    "    inputs, targets = prepare_data(video_data)\n",
    "    train_dataset = torch.utils.data.TensorDataset(inputs, targets)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 4. Initialize model, loss function, and optimizer\n",
    "    model = AutoregressiveVideoModel(channels, hidden_channels, channels)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # 5. Train the model\n",
    "    train_model(model, train_loader, optimizer, loss_fn, epochs)\n",
    "\n",
    "    # 6. Generate a future frame\n",
    "    # Example: Use the first 3 frames from the first video in the batch to predict the 4th frame\n",
    "    past_frames = video_data[:1, :3, :, :, :]  # (1, 3, 3, 64, 64)\n",
    "    next_frame = generate_next_frame(model, past_frames)\n",
    "    print(\"Predicted next frame shape:\", next_frame.shape)  # Should be (1, 3, 64, 64)\n",
    "\n",
    "    # 7.  Visualize the input frames and the predicted next frame.\n",
    "    #     (Requires matplotlib)\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def visualize_frames(frames, title=\"Frames\"):\n",
    "        \"\"\"\n",
    "        Visualizes a sequence of frames using matplotlib.\n",
    "\n",
    "        Args:\n",
    "            frames (torch.Tensor): Tensor of shape (1, num_frames, channels, height, width).\n",
    "            title (str): Title of the plot.\n",
    "        \"\"\"\n",
    "        frames = frames.squeeze(0).permute(0, 2, 3, 1).cpu().numpy() # (num_frames, height, width, channels)\n",
    "        num_frames_to_plot = frames.shape[0]\n",
    "        fig, axes = plt.subplots(1, num_frames_to_plot, figsize=(15, 5))\n",
    "        fig.suptitle(title)\n",
    "        for i in range(num_frames_to_plot):\n",
    "            axes[i].imshow(frames[i])\n",
    "            axes[i].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # Visualize the input frames and the prediction\n",
    "    visualize_frames(past_frames, title=\"Past Frames\")\n",
    "    visualize_frames(next_frame.unsqueeze(0), title=\"Predicted Next Frame\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tutorial]",
   "language": "python",
   "name": "conda-env-tutorial-py"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
